{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kreshuklab/teaching-dl-course-2019/blob/master/Webinars/exercise3/unet_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5CNxOpgx-ey",
        "colab_type": "text"
      },
      "source": [
        "## The libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IK0BTeevRv_N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "%load_ext tensorboard\n",
        "import os\n",
        "import imageio\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torchvision import transforms, utils\n",
        "from scipy.ndimage import binary_erosion"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFTBWHvqLYwL",
        "colab_type": "text"
      },
      "source": [
        "## Data loading and preprocessing\n",
        "\n",
        "For this exercise we will be using the Kaggle 2018 Data Science Bowl data again, but this time we will try to segment it with the state of the art network.\n",
        "Let's start with loading the data as before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k82ftNngK8Qf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1lEPEXGIxYeheiaHAp2G8rIK6Y3BGzVNN' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1lEPEXGIxYeheiaHAp2G8rIK6Y3BGzVNN\" -O kaggle_data.zip && rm -rf /tmp/cookies.txt\n",
        "!unzip -qq kaggle_data.zip && rm kaggle_data.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89JYxq-bL-Q3",
        "colab_type": "text"
      },
      "source": [
        "Now make sure that the data was successfully extracted: if everything went fine, you should have folders `nuclei_train_data` and `nuclei_val_data` in your working directory. Check if it is the case:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dk-fAyl7L-Zj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls -ltrh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuRXDRJsMxVo",
        "colab_type": "text"
      },
      "source": [
        "__TASK__: Use `ls` to explore the contents of both folders. Running `ls your_folder_name` should display you what is stored in the folder of your interest.\n",
        "\n",
        " How are the images stored? What format do they have? What about the ground truth (the annotation masks)? Which format are they stored in?\n",
        "\n",
        "Hint: you can use the following function to display the images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehovgGj1NQrG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_one_image(image_path):\n",
        "  image = imageio.imread(image_path)\n",
        "  plt.imshow(image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttIQl_WZd5Lo",
        "colab_type": "text"
      },
      "source": [
        "What one would normally start with in any machine learning pipeline is writing a dataset - a class that will fetch the training samples. In the previous exercises we did not have to worry about it, since we used the classic datasets available in the torchvision library. However, once you switch to using your own data, you would have to figure out how to fetch the data yourself. Luckily most of the functionality is already provided by PyTorch, but what you need to do is to write a class, that will actually supply the dataloader with training samples - a Dataset.\n",
        "\n",
        "Please take a moment to read about it [here](https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset) and [here](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class).\n",
        "\n",
        "The main idea: any Dataset class should have two methods: __len__ that returns the dataset length (the number of element) and __getitem__ that, given an index, returns input (image) and target (ground truth).\n",
        "\n",
        "For this exercise you will not have to do it yourself yet, but please carefully read through the provided class:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6Zf2tpXh8Tk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#any PyTorch dataset class should inherit the initial torch.utils.data.Dataset\n",
        "class NucleiDataset(Dataset):\n",
        "    \"\"\" A PyTorch dataset to load cell images and nuclei masks \"\"\"\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir  # the directory with all the training samples\n",
        "        self.samples = os.listdir(root_dir) # list the samples\n",
        "        self.transform = transform    # transformations to apply to both inputs and targets\n",
        "        #  transformations to apply just to inputs\n",
        "        self.inp_transforms = transforms.Compose([transforms.Grayscale(), # some of the images are RGB\n",
        "                                                  transforms.ToTensor(),\n",
        "                                                  transforms.Normalize([0.5], [0.5])\n",
        "                                                  ])\n",
        "        # transformations to apply just to targets\n",
        "        self.mask_transforms = transforms.ToTensor()\n",
        "\n",
        "    # get the total number of samples\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    # fetch the training sample given its index\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.root_dir, self.samples[idx],\n",
        "                                'images', self.samples[idx]+'.png')\n",
        "        # we'll be using Pillow library for reading files\n",
        "        # since many torchvision transforms operate on PIL images \n",
        "        image = Image.open(img_path)\n",
        "        image = self.inp_transforms(image)\n",
        "        masks_dir = os.path.join(self.root_dir, self.samples[idx], 'masks')\n",
        "        # masks directory has multiple images - one mask per nucleus\n",
        "        masks_list = os.listdir(masks_dir)\n",
        "        # create an empty array\n",
        "        mask = torch.zeros(1, len(image[0]), len(image[0][0]))\n",
        "        # iterate through the images to sum them up to one mask\n",
        "        for mask_name in masks_list:\n",
        "            one_nuclei_mask = Image.open(os.path.join(masks_dir, mask_name))\n",
        "            # erode the image by one pixel\n",
        "            # TASK: guess why we are doing it\n",
        "            one_nuclei_mask = binary_erosion(one_nuclei_mask)\n",
        "            one_nuclei_mask = self.mask_transforms(one_nuclei_mask)\n",
        "            # add this nucleus to the mask\n",
        "            mask += one_nuclei_mask\n",
        "        if self.transform is not None:\n",
        "            image, mask = self.transform([image, mask])\n",
        "        return image, mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wp6nJdOgvZBl",
        "colab_type": "text"
      },
      "source": [
        "Now let's load the dataset and visualize it with a simple function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCjyh0Rbvf6Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_DATA_PATH = 'nuclei_train_data'\n",
        "train_data = NucleiDataset(TRAIN_DATA_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fq0GitfQvnJF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_dataset(dataset):\n",
        "    idx = np.random.randint(0, len(dataset))    # take a random sample\n",
        "    img, mask = dataset[idx]                    # get the image and the nuclei masks\n",
        "    f, axarr = plt.subplots(1, 2)               # make two plots on one figure\n",
        "    axarr[0].imshow(img[0])                     # show the image\n",
        "    axarr[1].imshow(mask[0])                    # show the masks\n",
        "    _ = [ax.axis('off') for ax in axarr]        # remove the axes\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYcY1Tg9vpLJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_dataset(train_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMHvV2rtvqyX",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "As you can probably see, if you clicked enough times, some of the images are really huge! What happens if we load them into memory and run the model on them? We might run out of memory. That's why normally, when training networks on images or volumes one has to be really careful about the sizes. In practice, you would want to regulate their size. Additional reason for restraining the size is: if we want to train in batches (faster and more stable training), we need all the images in the batch to be of the same size. That is why we prefer to either resize or crop them.\n",
        "\n",
        "Here is a function (well, actually a class), that will apply a transformation 'random crop'. Notice that we apply it to images and masks simultaneously to make sure they correspond, despite the randomness.\n",
        "\n",
        "In case anybody is wondering why we have to bother to write a whole class for it instead of simply coping the images directly in the dataset: we want to keep the code modular. We want to write one dataset object, and then we can try all the possible transforms with this one dataset. Similarly, we want to write one Randomcrop transform object, and then we can reuse it for any other image datasets we night have in the future.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3X8AbTxGvyc1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RandomCrop(object):\n",
        "    \"\"\"Crop randomly the input image and the output mask\"\"\"\n",
        "    def __init__(self, crop_size):\n",
        "        # check if the crop size is of a valid type\n",
        "        assert isinstance(crop_size, (int, tuple, list))\n",
        "        if isinstance(crop_size, int):\n",
        "            # if the crop size is an integer, we use the same for both dimensions\n",
        "            self.output_size = (crop_size, crop_size)\n",
        "        else:\n",
        "            assert len(crop_size) == 2\n",
        "            self.crop_size = crop_size\n",
        "\n",
        "    # this function makes our class callable \n",
        "    def __call__(self, sample):\n",
        "        # we need to crop both input and mask at the same time\n",
        "        assert len(sample) == 2\n",
        "        image, mask = sample\n",
        "        # the first dimension is channels, then width, then height\n",
        "        w, h = image.shape[1:]\n",
        "        new_w, new_h = self.output_size\n",
        "        # choose a random place to crop\n",
        "        top = np.random.randint(0, h - new_h) if h - new_h > 0 else 0\n",
        "        left = np.random.randint(0, w - new_w) if w - new_w > 0 else 0\n",
        "        # crop and return\n",
        "        image = image[:, left: left + new_w, top: top + new_h]\n",
        "        mask = mask[:, left: left + new_w, top: top + new_h]\n",
        "        return image, mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLA-L6sZw5TZ",
        "colab_type": "text"
      },
      "source": [
        "PS: PyTorch already has quite a bunch of all possible data transforms, so if you need one, check [here](https://pytorch.org/docs/stable/torchvision/transforms.html). The biggest problem with them is that they are clearly separated into transforms applied to PIL images (remember, we initially load the images as PIL.Image?) and torch.tensors (remember, we converted the images into tensors by calling transforms.ToTensor()?). This can be incredibly annoying if for some reason you might need to transorm your images to tensors before applying any other transforms or you don't want to use PIL library at all."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQ5hBaPNxJG1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = NucleiDataset(TRAIN_DATA_PATH, RandomCrop(256))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FX2SMLSCxNDy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_dataset(train_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVH6172mxP0c",
        "colab_type": "text"
      },
      "source": [
        "And the same for the validation data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qsrwcz9axSLG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VAL_DATA_PATH = 'nuclei_val_data'\n",
        "val_data = NucleiDataset(VAL_DATA_PATH, RandomCrop(256))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxltUmVLxazR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_dataset(val_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pfxGQq2yhkD",
        "colab_type": "text"
      },
      "source": [
        "## The model: U-net\n",
        "\n",
        "Now we need to define the architecture of the model to use. This time we will use a [U-Net](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/) that has proven to steadily outperform the other architectures in segmenting biological and medical images.\n",
        "\n",
        "The image of the model precisely describes all the building blocks you need to use to create it. All of them can be found in the list of PyTorch layers (modules) [here](https://pytorch.org/docs/stable/nn.html#convolution-layers).\n",
        "\n",
        "The U-net has an encoder-decoder structure:\n",
        "\n",
        "In the encoder pass, the input image is successively downsampled via max-pooling. In the decoder pass it is upsampled again via transposed convolutions.\n",
        "\n",
        "In adddition, it has skip connections, that bridge the output from an encoder to the corresponding decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lf7Y_PmU7sQ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}